# -*- coding: utf-8 -*-
"""Siamese_ResNet101_cbam2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmYh6yer8JJwmYc-HJzc7G0MVbrcvQwM
"""

import tensorflow as tf
from tensorflow.keras.applications import ResNet101V2
from tensorflow.keras import layers, Model
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import Sequence
from tensorflow.keras.callbacks import ModelCheckpoint

import pandas as pd
import numpy as np
import time
from tqdm import tqdm
from itertools import combinations
import random
import cv2
from sklearn.utils import shuffle
import os
import pathlib
import pickle

# from google.colab import files
# files.upload()

class ChannelAttention(tf.keras.layers.Layer):
    def __init__(self, ratio=16, dense_initializer='glorot_uniform', bias_initializer='zeros'):
        super(ChannelAttention, self).__init__()
        self.ratio = ratio
        self.dense_initializer = dense_initializer
        self.bias_initializer = bias_initializer

    def build(self, input_shape):
        self.dense_1 = Dense(input_shape[-1] // self.ratio, activation='relu', kernel_initializer=self.dense_initializer, bias_initializer=self.bias_initializer)
        self.dense_2 = Dense(input_shape[-1], activation='sigmoid', kernel_initializer=self.dense_initializer, bias_initializer=self.bias_initializer)

    def call(self, inputs):
        avg_pool = GlobalAveragePooling2D()(inputs)
        max_pool = GlobalMaxPooling2D()(inputs)
        avg_pool = self.dense_1(avg_pool)
        max_pool = self.dense_1(max_pool)
        avg_pool = self.dense_2(avg_pool)
        max_pool = self.dense_2(max_pool)
        attention = Add()([avg_pool, max_pool])
        return Multiply()([inputs, attention])

class SpatialAttention(tf.keras.layers.Layer):
    def __init__(self, conv_initializer='glorot_uniform', bias_initializer='zeros'):
        super(SpatialAttention, self).__init__()
        self.conv_initializer = conv_initializer
        self.bias_initializer = bias_initializer

    def build(self, input_shape):
        self.conv = Conv2D(1, (7, 7), padding='same', activation='sigmoid', kernel_initializer=self.conv_initializer, bias_initializer=self.bias_initializer)

    def call(self, inputs):
        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)
        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)
        attention = tf.concat([avg_pool, max_pool], axis=-1)
        attention = self.conv(attention)
        return Multiply()([inputs, attention])

class CBAM(Model):
    def __init__(self, base_model):
        super(CBAM, self).__init__()
        self.base_model = base_model
        self.channel_attention = ChannelAttention()
        self.spatial_attention = SpatialAttention()

    def call(self, inputs):
        x = self.base_model(inputs)
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x

def resnet_cbam(input_shape=(300, 300, 3)):
    resnet101v2_base = ResNet101V2(include_top=False, input_shape=input_shape, weights='imagenet')
    for layer in resnet101v2_base.layers:
      layer.trainable = False
    cbam_resnet101v2 = CBAM(resnet101v2_base)

    inputs = tf.keras.Input(shape=input_shape)
    x = cbam_resnet101v2(inputs)
    x = GlobalAveragePooling2D()(x)
    outputs = tf.keras.layers.Dense(2048, activation='relu')(x)


    model = Model(inputs=inputs, outputs=outputs)

    return model

def Siamese(input_shape):

  # Define the two input layers
  input_1 = tf.keras.layers.Input(shape=input_shape)
  input_2 = tf.keras.layers.Input(shape=input_shape)
  
  # Define the Subnet model
  # subnet = Subnet(input_shape, weights)
  subnet = resnet_cbam(input_shape)

  # Get the output of the Subnet model for the two input layers
  output_1 = subnet(input_1)
  output_2 = subnet(input_2)

  # Define the L2 distance layer
  dist_layer = tf.keras.layers.Lambda(lambda x: K.sqrt(K.sum(K.square(x[0] - x[1]), axis=-1, keepdims=False)))
  distance = dist_layer([output_1, output_2])

  # Define the Siamese model
  siamese_model = Model(inputs=[input_1, input_2], outputs=distance)
  
  # Compile the Siamese model
  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)
  siamese_model.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[contrastive_accuracy])
  
  return siamese_model

def contrastive_loss(y_true, pred_dist, margin=100000.0):
  loss = K.mean(y_true * K.square(pred_dist) + (1 - y_true) * K.square(K.maximum(margin - pred_dist, 0)))
  return loss

def contrastive_accuracy(y_true, pred_dist):
  accuracy = K.mean(K.equal(y_true, K.cast(pred_dist < 100000, y_true.dtype)))
  return accuracy

def pair_generator(df, tqdm_desc=None, data_type='train'):
    landmark_groups = df.groupby('landmark_id')
    
    pair_list = []
    single_list = []
    
    if tqdm_desc == None:
      tqdm_desc = "Generating pairs"
    total_classes = len(landmark_groups)
    
    # Iterate through each landmark group and create similar pairs within the group
    for _, group in tqdm(landmark_groups, desc=tqdm_desc, total=total_classes):
        ids = group['id'].tolist()
        random.shuffle(ids)
        single_list.append(ids[0])
        if len(ids) > 1:
          if len(ids) % 2 != 0:
            ids.remove(ids[0])     
          for i in range(0, len(ids), 2):
            pair_list.append({"id1": ids[i], "id2": ids[i+1], "similarity": 1})

    if len(single_list) % 2 != 0:
      random_idx = random.randint(0, len(single_list)-2)
      single_list.append(single_list[random_idx])

    dissimilar_pairs = []

    if data_type == 'train':
      num_iter = 5
    else:
      num_iter = 1

    for i in tqdm(range(num_iter), desc=tqdm_desc):
      random.shuffle(single_list)
      for j in range(0, len(single_list)-1):
        dissimilar_pairs.append({"id1": single_list[j], "id2": single_list[j+1], "similarity": 0})

    pair_list.extend(dissimilar_pairs)
    random.shuffle(pair_list)
    pair_df = pd.DataFrame(pair_list)
    return pair_df

# input_shape = (300, 300, 3)
# train_batch_size, valid_batch_size = 256, 128
# target_size=input_shape[:2]
# num_epochs=1
# buffer_size=100

# siamese_model = Siamese(input_shape)
# siamese_model.summary()
# subnet_model = siamese_model.layers[2]
# subnet_model.summary()

def process_image(file_path):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [300, 300])
    img /= 255.0  # Normalize pixel values to [0,1]
    return img

def load_image_pair_and_label(row):
    data_dir = './kaggle_datasets/train_dir/'
    img1_path = str(data_dir) + '/' + row['id1']
    img2_path = str(data_dir) + '/' + row['id2']
    img1 = process_image(img1_path)
    img2 = process_image(img2_path)
    label = tf.cast(row['similarity'], tf.float32)
    return (img1, img2), label

def split_df(df, num_splits):
    return np.array_split(df, num_splits)

def random_val_sample(val_dataset, sample_size, buffer_size):
    val_sample = val_dataset.shuffle(buffer_size).take(sample_size).batch(sample_size)
    return val_sample

class Valid_Saveparams(tf.keras.callbacks.Callback):
    def __init__(self, val_dataset, sample_size=100, accuracy_delta=0.0001, val_interval=100, buffer_size=1000):
        self.val_dataset = val_dataset
        self.sample_size = sample_size
        self.accuracy_delta = accuracy_delta
        self.validation_interval = val_interval
        self.buffer_size =  buffer_size
        super().__init__()

    def on_train_begin(self, logs=None):
        self.batch_losses = []
        self.batch_accs = []
        self.val_batch_accs = []

    def on_train_batch_end(self, batch, logs=None):
        self.batch_losses.append(logs['loss'])
        self.batch_accs.append(logs['contrastive_accuracy'])

        if batch % self.validation_interval == 0:
          val_sample = random_val_sample(self.val_dataset, self.sample_size, self.buffer_size)
          val_metrics = self.model.evaluate(val_sample, verbose=0)
          self.val_batch_accs.append(val_metrics[1])
          print(f' Validation accuracy: {val_metrics[1]}')
          self.model.save_weights(f"./model_weights/ft3/siamese_weights_101_cbam{batch}.h5")

        if len(self.val_batch_accs) > 1 and abs(self.val_batch_accs[-1] - self.val_batch_accs[-2]) < self.accuracy_delta:
            print(f"Change in validation accuracy is below {self.accuracy_delta * 100:.2f}%, stopping training.")
            self.model.stop_training = True

input_shape = (300, 300, 3)
train_batch_size, valid_batch_size = 256, 128
target_size=input_shape[:2]
num_epochs=1
buffer_size=1000


image_dir = pathlib.Path('./kaggle_datasets/train_dir/')
train_csv_path = './kaggle_datasets/train_split.csv'
valid_csv_path = './kaggle_datasets/test_split.csv'

train_df = pd.read_csv(train_csv_path)
valid_df = pd.read_csv(valid_csv_path)

train_pairs_df = pair_generator(train_df, tqdm_desc="Generating Train pairs", data_type='train')
valid_pairs_df = pair_generator(valid_df, tqdm_desc="Generating Valid pairs", data_type='valid')

train_dataset = tf.data.Dataset.from_tensor_slices(dict(train_pairs_df))
val_dataset = tf.data.Dataset.from_tensor_slices(dict(valid_pairs_df))

train_dataset = train_dataset.map(load_image_pair_and_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)
val_dataset = val_dataset.map(load_image_pair_and_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)

train_ds = train_dataset.shuffle(buffer_size).batch(train_batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
val_ds = val_dataset.batch(valid_batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

siamese_model = Siamese(input_shape)

callback = Valid_Saveparams(val_dataset)

siamese_model.fit(train_ds, epochs=num_epochs, callbacks=[callback])

siamese_model.save_weights("./model_weights/siamese_weights_101_cbam.h5")
print("Weights saved!")

with open('batch_history_101_cbam.pkl', 'wb') as f:
    pickle.dump({'loss': callback.batch_losses, 'accuracy': callback.batch_accs, 'val_accuracy': callback.val_batch_accs}, f)

# # Load the batch history data from the pickle file
# with open('batch_history.pkl', 'rb') as f:
#     loaded_data = pickle.load(f)

# loaded_losses = loaded_data['loss']
# loaded_accuracies = loaded_data['accuracy']

val_loss, val_contrastive_accuracy = siamese_model.evaluate(val_ds)

print(f"Validation loss: {val_loss:.4f}")
print(f"Validation contrastive accuracy: {val_contrastive_accuracy * 100:.2f}%")